{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Fine-tune BGE-M3 for Arabic Islamic Texts (Memory-Optimized)\n\nThis notebook fine-tunes BAAI/bge-m3 on Quran and Hadith data for improved Arabic Islamic text search.\n\n## Memory-Optimized Strategy\n- **CachedMultipleNegativesRankingLoss** - properly handles hard negatives\n- **Gradient accumulation** (batch 4 √ó 8 = 32 effective)\n- **Sequence length limit** (256 tokens) for memory efficiency\n- **3 hard negatives per query** (similarity 0.65-0.85 zone)\n- **Target metrics**: Precision@5 > 0.85, MRR > 0.80\n\n## Instructions\n1. Go to **Runtime ‚Üí Change runtime type ‚Üí Select H100 GPU**\n2. Upload your training files when prompted:\n   - `combined_training.jsonl` (required - 161MB with hard negatives)\n   - `gold_standard_evaluation.jsonl` (recommended)\n3. Run all cells\n4. Download the fine-tuned model at the end\n\n## Verification Checklist\n- [ ] GPU memory should stay under 30GB (check after first training step)\n- [ ] Initial loss should be ~2.5-4.0 (NOT 0.000000)\n- [ ] Loss should decrease over training\n\n**Estimated time**: ~30-45 minutes on H100",
   "metadata": {
    "id": "intro"
   }
  },
  {
   "cell_type": "code",
   "source": "# Install required packages\n!pip install -q sentence-transformers>=3.4.0 datasets accelerate",
   "metadata": {
    "id": "install"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be slow.\")\n",
    "    DEVICE = \"cpu\""
   ],
   "metadata": {
    "id": "gpu_check"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Upload training data file\nfrom google.colab import files\n\nprint(\"Please upload your training files:\")\nprint(\"=\" * 50)\nprint(\"\\nREQUIRED:\")\nprint(\"  - combined_training.jsonl (training data with hard negatives)\")\nprint(\"\\nRECOMMENDED:\")\nprint(\"  - gold_standard_evaluation.jsonl (200 curated test queries)\")\nprint(\"\\nThese are in: training/data/\")\nprint(\"\\nYou can select multiple files at once.\")\n\nuploaded = files.upload()\n\n# Check what was uploaded\nhas_eval_file = 'gold_standard_evaluation.jsonl' in uploaded\nhas_training_file = 'combined_training.jsonl' in uploaded\n\nif has_training_file:\n    print(\"\\n‚úì Training data uploaded\")\nelse:\n    print(\"\\n‚ö† Warning: combined_training.jsonl not found\")\n    \nif has_eval_file:\n    print(\"‚úì Gold standard evaluation set uploaded (will evaluate during training)\")\nelse:\n    print(\"‚ö† No evaluation file - training will proceed without checkpointing\")",
   "metadata": {
    "id": "upload"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load training data\nimport json\nfrom sentence_transformers import InputExample\n\n# Configuration - Keep all 3 hard negatives for best quality\nMAX_HARD_NEGATIVES = 3\n\ndef load_jsonl(filename, max_negatives=3):\n    \"\"\"Load training pairs from JSONL, handling hard negatives if present\"\"\"\n    examples = []\n    has_negatives = False\n\n    with open(filename, 'r', encoding='utf-8') as f:\n        for line in f:\n            if not line.strip():\n                continue\n\n            data = json.loads(line)\n            query = data.get('query', '')\n            positives = data.get('pos', [])\n            negatives = data.get('neg', [])\n\n            if not query or not positives:\n                continue\n\n            if negatives:\n                has_negatives = True\n\n            for pos in positives:\n                if negatives:\n                    # With hard negatives: [query, positive, neg1, neg2, neg3]\n                    neg_subset = negatives[:max_negatives]\n                    texts = [query, pos] + neg_subset\n                    examples.append(InputExample(texts=texts))\n                else:\n                    # Without negatives: [query, positive]\n                    examples.append(InputExample(texts=[query, pos]))\n\n    return examples, has_negatives\n\n# Load all uploaded files\ntrain_examples = []\nhas_hard_negatives = False\n\nfor filename in uploaded.keys():\n    if filename.endswith('.jsonl') and 'evaluation' not in filename:\n        print(f\"Loading {filename}...\")\n        examples, has_neg = load_jsonl(filename, MAX_HARD_NEGATIVES)\n        train_examples.extend(examples)\n        has_hard_negatives = has_hard_negatives or has_neg\n        print(f\"  Loaded {len(examples)} examples\")\n\nprint(f\"\\nTotal training examples: {len(train_examples)}\")\nprint(f\"Hard negatives present: {has_hard_negatives}\")\nif has_hard_negatives:\n    print(f\"Texts per example: {len(train_examples[0].texts)} (query + pos + {MAX_HARD_NEGATIVES} neg)\")",
   "metadata": {
    "id": "load_data"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load BGE-M3 model\nimport gc\nfrom sentence_transformers import SentenceTransformer\nfrom torch.utils.data import DataLoader\nimport random\n\n# Clear any existing memory\ngc.collect()\ntorch.cuda.empty_cache()\n\nMODEL_NAME = 'BAAI/bge-m3'\n\n# Memory optimization: limit sequence length\nMAX_SEQ_LENGTH = 256  # Reduces memory significantly (default is often 512+)\n\nprint(f\"Loading {MODEL_NAME} model...\")\nmodel = SentenceTransformer(MODEL_NAME, device=DEVICE)\n\n# Apply sequence length limit for memory optimization\nmodel.max_seq_length = MAX_SEQ_LENGTH\n\nprint(f\"Model loaded on: {model.device}\")\nprint(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")\nprint(f\"Max sequence length: {model.max_seq_length}\")\nprint(f\"GPU memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\nprint(f\"GPU memory available: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1e9:.1f} GB\")",
   "metadata": {
    "id": "load_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Training configuration (OPTIMIZED FOR H100 80GB - MEMORY-SAFE)\nfrom sentence_transformers.losses import CachedMultipleNegativesRankingLoss\n\n# Memory-optimized settings with gradient accumulation\nBATCH_SIZE = 4                    # Small batch for memory safety\nGRADIENT_ACCUMULATION_STEPS = 8   # Effective batch = 4 √ó 8 = 32\nMINI_BATCH_SIZE = 16              # Internal batching for CachedMNRL\n\nEPOCHS = 2              # 2 epochs with quality data\nLEARNING_RATE = 1e-5    # Conservative learning rate\nWARMUP_RATIO = 0.1      # 10% warmup\nOUTPUT_DIR = './arabic-islamic-bge-m3'\n\n# Shuffle training data\nrandom.shuffle(train_examples)\n\n# Use CachedMultipleNegativesRankingLoss - designed for hard negatives\n# This loss properly handles [query, pos, neg1, neg2, neg3] format\n# and caches embeddings to reduce memory during loss computation\ntrain_loss = CachedMultipleNegativesRankingLoss(\n    model,\n    mini_batch_size=MINI_BATCH_SIZE,\n)\n\n# Create dataloader with small batch size\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)\n\n# Calculate steps (accounting for gradient accumulation)\nsteps_per_epoch = len(train_dataloader)\neffective_batch_size = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\ntotal_steps = steps_per_epoch * EPOCHS\nwarmup_steps = int(total_steps * WARMUP_RATIO)\n\n# Calculate negatives\nin_batch_negatives = effective_batch_size - 1\ntotal_negatives = in_batch_negatives + MAX_HARD_NEGATIVES\n\nprint(f\"\\n{'='*60}\")\nprint(\"MEMORY-OPTIMIZED Training Configuration (H100 80GB)\")\nprint(f\"{'='*60}\")\nprint(f\"  GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.0f} GB\")\nprint(f\"  Max sequence length: {MAX_SEQ_LENGTH}\")\nprint(f\"  Actual batch size: {BATCH_SIZE}\")\nprint(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"  Effective batch size: {effective_batch_size}\")\nprint(f\"  Loss function: CachedMultipleNegativesRankingLoss\")\nprint(f\"  Mini-batch size (for loss): {MINI_BATCH_SIZE}\")\nprint(f\"  In-batch negatives: {in_batch_negatives}\")\nprint(f\"  Hard negatives: {MAX_HARD_NEGATIVES}\")\nprint(f\"  Total negatives per sample: {total_negatives}\")\nprint(f\"  Epochs: {EPOCHS}\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Warmup steps: {warmup_steps}\")\nprint(f\"  Steps per epoch: {steps_per_epoch}\")\nprint(f\"  Total steps: {total_steps}\")\nprint(f\"  Training examples: {len(train_examples)}\")\nprint(f\"  Mixed precision (FP16): ENABLED\")\nprint(f\"{'='*60}\")\nprint(f\"\\nüí° Memory estimate: ~20-30GB (vs 80GB+ before)\")\nprint(f\"   Check after first step: print(f'GPU: {{torch.cuda.memory_allocated()/1e9:.1f}}GB')\")",
   "metadata": {
    "id": "config"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Train with SentenceTransformerTrainer (supports gradient accumulation)\nimport os\nfrom datasets import Dataset\nfrom sentence_transformers import SentenceTransformerTrainingArguments, SentenceTransformerTrainer\nfrom sentence_transformers.training_args import BatchSamplers\n\nOUTPUT_DIR = './arabic-islamic-bge-m3'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"Starting training on H100...\")\nprint(f\"  {len(train_examples)} examples\")\nprint(f\"  Batch size: {BATCH_SIZE} √ó {GRADIENT_ACCUMULATION_STEPS} = {effective_batch_size} effective\")\nprint(f\"  {total_negatives} negatives per sample ({in_batch_negatives} in-batch + {MAX_HARD_NEGATIVES} hard)\")\nprint(f\"  Loss: CachedMultipleNegativesRankingLoss (handles hard negatives properly)\")\nprint(f\"\\n‚ö†Ô∏è  IMPORTANT: Initial loss should be ~2.5-4.0 (NOT 0.000000)\")\nprint(f\"   If loss = 0.000, stop training - something is wrong.\\n\")\n\n# Convert InputExample list to HuggingFace Dataset\n# SentenceTransformerTrainer expects columns: sentence_0, sentence_1, ...\ndef convert_to_dataset(examples):\n    \"\"\"Convert list of InputExample to HuggingFace Dataset\"\"\"\n    # Find the expected number of texts (max across all examples)\n    expected_num_texts = max(len(ex.texts) for ex in examples)\n    \n    # Filter to only include examples with the expected number of texts\n    # (CachedMultipleNegativesRankingLoss needs consistent format)\n    filtered_examples = [ex for ex in examples if len(ex.texts) == expected_num_texts]\n    \n    if len(filtered_examples) < len(examples):\n        print(f\"  Filtered {len(examples) - len(filtered_examples)} examples with inconsistent text count\")\n        print(f\"  Using {len(filtered_examples)} examples with {expected_num_texts} texts each\")\n    \n    data = {}\n    for i in range(expected_num_texts):\n        data[f\"sentence_{i}\"] = [ex.texts[i] for ex in filtered_examples]\n    \n    return Dataset.from_dict(data)\n\ntrain_dataset = convert_to_dataset(train_examples)\nprint(f\"Converted to Dataset with columns: {train_dataset.column_names}\")\nprint(f\"Dataset size: {len(train_dataset)} examples\")\n\n# Training arguments with gradient accumulation\ntraining_args = SentenceTransformerTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    learning_rate=LEARNING_RATE,\n    warmup_steps=warmup_steps,  # Use warmup_steps instead of deprecated warmup_ratio\n    fp16=True,  # Mixed precision for memory efficiency\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    save_total_limit=1,  # Only keep latest checkpoint\n    batch_sampler=BatchSamplers.NO_DUPLICATES,\n    report_to=\"none\",  # Disable wandb/tensorboard\n)\n\n# Create trainer with proper Dataset\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    loss=train_loss,\n)\n\n# Train\ntrainer.train()\n\n# Check GPU memory after training\nprint(f\"\\n{'='*60}\")\nprint(\"‚úì Training complete!\")\nprint(f\"{'='*60}\")\nprint(f\"Final GPU memory: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\nprint(f\"Model saved to: {OUTPUT_DIR}\")\n\n# Save the final model explicitly\nmodel.save(OUTPUT_DIR)",
   "metadata": {
    "id": "train"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save training configuration\nimport json\n\nconfig = {\n    \"model_name\": MODEL_NAME,\n    \"batch_size\": BATCH_SIZE,\n    \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n    \"effective_batch_size\": BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS,\n    \"max_seq_length\": MAX_SEQ_LENGTH,\n    \"epochs\": EPOCHS,\n    \"learning_rate\": LEARNING_RATE,\n    \"warmup_steps\": warmup_steps,\n    \"warmup_ratio\": WARMUP_RATIO,\n    \"loss_function\": \"CachedMultipleNegativesRankingLoss\",\n    \"mini_batch_size\": MINI_BATCH_SIZE,\n    \"hard_negatives_per_query\": MAX_HARD_NEGATIVES,\n    \"in_batch_negatives\": effective_batch_size - 1,\n    \"total_negatives_per_sample\": effective_batch_size - 1 + MAX_HARD_NEGATIVES,\n    \"num_training_examples\": len(train_examples),\n    \"fp16\": True,\n    \"gpu\": torch.cuda.get_device_name(0),\n    \"strategy\": \"memory-optimized-h100\",\n    \"target_metrics\": {\n        \"precision_at_5\": \"> 0.85\",\n        \"mrr\": \"> 0.80\",\n        \"false_positive_rate\": \"< 15%\"\n    }\n}\n\nwith open(f'{OUTPUT_DIR}/training_config.json', 'w') as f:\n    json.dump(config, f, indent=2)\n\nprint(\"Training configuration saved:\")\nprint(json.dumps(config, indent=2))",
   "metadata": {
    "id": "save_config"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Test the fine-tuned model\n",
    "print(\"Testing fine-tuned model...\\n\")\n",
    "\n",
    "test_queries = [\n",
    "    # Arabic queries\n",
    "    \"ÿ•ŸÜŸÖÿß ÿßŸÑÿ£ÿπŸÖÿßŸÑ ÿ®ÿßŸÑŸÜŸäÿßÿ™\",  # Actions are by intentions\n",
    "    \"ÿßŸÑÿµŸÑÿßÿ© ŸÅŸä ŸàŸÇÿ™Ÿáÿß\",  # Prayer on time\n",
    "    \"ÿ¢Ÿäÿ© ÿßŸÑŸÉÿ±ÿ≥Ÿä\",  # Ayat al-Kursi\n",
    "    \"ŸÖÿß ÿ≠ŸÉŸÖ ÿßŸÑÿµŸäÿßŸÖ ŸÅŸä ÿ±ŸÖÿ∂ÿßŸÜÿü\",  # What is the ruling on fasting in Ramadan?\n",
    "    # English queries\n",
    "    \"What is the reward for patience?\",\n",
    "    \"hadith about charity\",\n",
    "    \"fasting in Ramadan\",\n",
    "    \"importance of good intentions\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    embedding = model.encode(query)\n",
    "    print(f\"'{query[:50]}' ‚Üí {len(embedding)}-dim vector\")"
   ],
   "metadata": {
    "id": "test"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Optional: Quick evaluation with sample queries\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Test semantic similarity\n",
    "test_pairs = [\n",
    "    # Should be similar\n",
    "    (\"Actions are judged by intentions\", \"ÿ•ŸÜŸÖÿß ÿßŸÑÿ£ÿπŸÖÿßŸÑ ÿ®ÿßŸÑŸÜŸäÿßÿ™\"),\n",
    "    (\"What does Islam say about patience?\", \"ÿßŸÑÿµÿ®ÿ± ŸÅŸä ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖ\"),\n",
    "    # Should be less similar\n",
    "    (\"Actions are judged by intentions\", \"ÿßŸÑÿµŸÑÿßÿ© ŸÅŸä ŸàŸÇÿ™Ÿáÿß\"),\n",
    "]\n",
    "\n",
    "print(\"Semantic similarity test:\\n\")\n",
    "for q1, q2 in test_pairs:\n",
    "    e1 = model.encode(q1)\n",
    "    e2 = model.encode(q2)\n",
    "    sim = cosine_similarity(e1, e2)\n",
    "    print(f\"'{q1[:30]}...' vs '{q2[:30]}...': {sim:.4f}\")"
   ],
   "metadata": {
    "id": "evaluate"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Evaluate on Gold Standard (if uploaded)\nimport numpy as np\nfrom collections import defaultdict\n\ndef evaluate_on_gold_standard(model, eval_file='gold_standard_evaluation.jsonl'):\n    \"\"\"Evaluate model on gold standard queries, computing Precision@K and MRR.\"\"\"\n    import os\n    if not os.path.exists(eval_file):\n        print(f\"‚ö† Evaluation file not found: {eval_file}\")\n        print(\"Upload gold_standard_evaluation.jsonl for detailed evaluation.\")\n        return None\n    \n    print(f\"\\n{'='*50}\")\n    print(\"Gold Standard Evaluation\")\n    print(f\"{'='*50}\\n\")\n    \n    # Load evaluation queries\n    eval_queries = []\n    with open(eval_file, 'r', encoding='utf-8') as f:\n        for line in f:\n            if line.strip():\n                eval_queries.append(json.loads(line))\n    \n    print(f\"Loaded {len(eval_queries)} evaluation queries\")\n    \n    # Load all training passages as the retrieval corpus\n    # (In a real evaluation, you'd use the actual Qdrant index)\n    corpus = []\n    corpus_ids = []\n    \n    # Use the training data as a simple corpus for evaluation\n    with open('combined_training.jsonl', 'r', encoding='utf-8') as f:\n        for line in f:\n            if line.strip():\n                data = json.loads(line)\n                for pos in data.get('pos', []):\n                    if pos not in corpus:\n                        corpus.append(pos)\n                        # Create a simple ID\n                        corpus_ids.append(f\"passage_{len(corpus_ids)}\")\n    \n    print(f\"Corpus size: {len(corpus)} passages\")\n    print(\"Encoding corpus (this may take a few minutes)...\")\n    \n    # Encode corpus\n    corpus_embeddings = model.encode(corpus, show_progress_bar=True, batch_size=32)\n    \n    # Evaluate each query\n    results = {\n        'precision_at_5': [],\n        'precision_at_10': [],\n        'mrr': [],\n        'by_category': defaultdict(list),\n        'by_difficulty': defaultdict(list),\n    }\n    \n    print(\"\\nEvaluating queries...\")\n    for eq in eval_queries:\n        query = eq['query']\n        relevant_ids = set(r['id'] for r in eq.get('relevant', []))\n        category = eq.get('category', 'unknown')\n        difficulty = eq.get('difficulty', 'medium')\n        \n        # Encode query\n        query_embedding = model.encode(query)\n        \n        # Compute similarities\n        similarities = np.dot(corpus_embeddings, query_embedding)\n        top_indices = np.argsort(similarities)[::-1][:10]\n        \n        # Since we don't have real IDs mapped, we'll use a simplified evaluation\n        # In practice, you'd map corpus_ids to the relevant_ids from gold standard\n        # Here we'll compute based on whether retrieved passages are \"correct\"\n        \n        # For now, just track the similarity scores for analysis\n        top_sims = [similarities[i] for i in top_indices]\n        \n        # Record metrics (simplified - full evaluation needs proper ID mapping)\n        results['precision_at_5'].append(np.mean(top_sims[:5]))\n        results['precision_at_10'].append(np.mean(top_sims[:10]))\n        results['mrr'].append(top_sims[0] if top_sims else 0)\n        results['by_category'][category].append(top_sims[0] if top_sims else 0)\n        results['by_difficulty'][difficulty].append(top_sims[0] if top_sims else 0)\n    \n    # Print results\n    print(f\"\\n{'='*50}\")\n    print(\"EVALUATION RESULTS\")\n    print(f\"{'='*50}\")\n    print(f\"\\nOverall Metrics (similarity-based proxy):\")\n    print(f\"  Avg Top-5 Similarity:  {np.mean(results['precision_at_5']):.4f}\")\n    print(f\"  Avg Top-10 Similarity: {np.mean(results['precision_at_10']):.4f}\")\n    print(f\"  Avg Top-1 Similarity:  {np.mean(results['mrr']):.4f}\")\n    \n    print(f\"\\nBy Category:\")\n    for cat, scores in sorted(results['by_category'].items()):\n        print(f\"  {cat}: {np.mean(scores):.4f} (n={len(scores)})\")\n    \n    print(f\"\\nBy Difficulty:\")\n    for diff, scores in sorted(results['by_difficulty'].items()):\n        print(f\"  {diff}: {np.mean(scores):.4f} (n={len(scores)})\")\n    \n    print(f\"\\n{'='*50}\")\n    print(\"Note: For full Precision@K and MRR evaluation, run the\")\n    print(\"evaluate-precision.ts script locally with your Qdrant index.\")\n    print(f\"{'='*50}\")\n    \n    return results\n\n# Run evaluation if gold standard was uploaded\nif has_eval_file:\n    eval_results = evaluate_on_gold_standard(model)\nelse:\n    print(\"‚ö† No gold standard file uploaded.\")\n    print(\"For detailed evaluation metrics, upload gold_standard_evaluation.jsonl\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Download the fine-tuned model\nimport shutil\nfrom google.colab import files\n\n# Create zip file\nprint(\"Creating zip file...\")\nshutil.make_archive('arabic-islamic-bge-m3', 'zip', OUTPUT_DIR)\n\nprint(\"Downloading fine-tuned model...\")\nprint(\"\\nAfter download:\")\nprint(\"1. Extract: unzip arabic-islamic-bge-m3.zip -d training/outputs/arabic-islamic-bge-m3/\")\nprint(\"2. Start server: CUSTOM_WEIGHTS_PATH=./training/outputs/arabic-islamic-bge-m3 python embedding-server/main.py\")\nprint(\"3. Regenerate: bun run scripts/generate-embeddings.ts --model=bge-m3\")\nprint(\"4. Evaluate: bun run training/scripts/evaluate-precision.ts --model=bge-m3\")\n\nfiles.download('arabic-islamic-bge-m3.zip')",
   "metadata": {
    "id": "download"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}