"""
Fine-tune BGE-M3 for Arabic Islamic Texts (Precision-Optimized)

Uses sentence-transformers to fine-tune BAAI/bge-m3 on:
- Quran ayah + English/Arabic translation pairs
- Hadith Arabic + English/Arabic translation pairs
- Synthetic queries generated by LLM
- Arabic paraphrases for same-language search
- Hard negatives for fine-grained discrimination

Precision-optimized settings:
- Smaller batch size (16) for harder in-batch negatives
- Lower learning rate (1e-5) for conservative fine-tuning
- More epochs (2) with quality data
- Checkpoint evaluation on gold standard
- Early stopping on validation precision

Usage:
    python training/scripts/train_bge_m3.py [options]

Options:
    --data-file=<path>   Training data file (default: combined_training.jsonl)
    --output-dir=<path>  Output directory (default: outputs/arabic-islamic-bge-m3)
    --batch-size=<n>     Batch size (default: 16, smaller = harder negatives)
    --epochs=<n>         Number of epochs (default: 2)
    --lr=<rate>          Learning rate (default: 1e-5, conservative)
    --warmup=<ratio>     Warmup ratio (default: 0.1)
    --hard-negatives=<n> Number of hard negatives to use per query (default: 3)
    --use-matryoshka     Use Matryoshka loss for multi-dim embeddings
    --eval-file=<path>   Gold standard evaluation file for checkpoints
    --eval-steps=<n>     Evaluate every N steps (default: 500)
    --early-stop         Enable early stopping on validation metric
    --cpu                Force CPU even if GPU available
    --resume=<path>      Resume from checkpoint

Environment:
    - Runs on GPU if available, otherwise CPU
    - Set WANDB_API_KEY for experiment tracking (optional)
"""

import os
import sys
import json
import argparse
import random
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import math

# Environment setup - do before importing torch
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")

import torch

# Detect device
def get_device() -> str:
    if torch.cuda.is_available():
        return "cuda"
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        # MPS can have memory issues, use with caution
        return "mps"
    return "cpu"

from sentence_transformers import SentenceTransformer, InputExample, losses
from sentence_transformers.evaluation import InformationRetrievalEvaluator
from torch.utils.data import DataLoader


@dataclass
class TrainingConfig:
    data_file: Path
    output_dir: Path
    batch_size: int = 16  # Smaller for precision (harder in-batch negatives)
    epochs: int = 2       # More epochs with quality data
    learning_rate: float = 1e-5  # Conservative (less forgetting)
    warmup_ratio: float = 0.1
    hard_negatives: int = 3
    use_matryoshka: bool = False
    device: str = "auto"
    resume_from: Optional[Path] = None
    model_name: str = "BAAI/bge-m3"
    eval_file: Optional[Path] = None
    eval_steps: int = 500
    early_stop: bool = False
    patience: int = 3  # For early stopping


def parse_args() -> TrainingConfig:
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Fine-tune BGE-M3 for Arabic Islamic texts")

    script_dir = Path(__file__).parent
    default_data = script_dir.parent / "data" / "combined_training.jsonl"
    default_output = script_dir.parent / "outputs" / "arabic-islamic-bge-m3"
    default_eval = script_dir.parent / "data" / "gold_standard_evaluation.jsonl"

    parser.add_argument("--data-file", type=Path, default=default_data,
                       help="Training data JSONL file")
    parser.add_argument("--output-dir", type=Path, default=default_output,
                       help="Output directory for model")
    parser.add_argument("--batch-size", type=int, default=16,
                       help="Training batch size (smaller = harder negatives)")
    parser.add_argument("--epochs", type=int, default=2,
                       help="Number of training epochs")
    parser.add_argument("--lr", type=float, default=1e-5,
                       help="Learning rate (conservative for fine-tuning)")
    parser.add_argument("--warmup", type=float, default=0.1,
                       help="Warmup ratio")
    parser.add_argument("--hard-negatives", type=int, default=3,
                       help="Number of hard negatives per query")
    parser.add_argument("--use-matryoshka", action="store_true",
                       help="Use Matryoshka loss for multi-dimension embeddings")
    parser.add_argument("--eval-file", type=Path, default=None,
                       help="Gold standard file for checkpoint evaluation")
    parser.add_argument("--eval-steps", type=int, default=500,
                       help="Evaluate every N steps")
    parser.add_argument("--early-stop", action="store_true",
                       help="Enable early stopping on validation metric")
    parser.add_argument("--cpu", action="store_true",
                       help="Force CPU usage")
    parser.add_argument("--resume", type=Path, default=None,
                       help="Resume from checkpoint")

    args = parser.parse_args()

    # Check for eval file
    eval_file = args.eval_file
    if eval_file is None and default_eval.exists():
        eval_file = default_eval

    return TrainingConfig(
        data_file=args.data_file,
        output_dir=args.output_dir,
        batch_size=args.batch_size,
        epochs=args.epochs,
        learning_rate=args.lr,
        warmup_ratio=args.warmup,
        hard_negatives=args.hard_negatives,
        use_matryoshka=args.use_matryoshka,
        device="cpu" if args.cpu else "auto",
        resume_from=args.resume,
        eval_file=eval_file,
        eval_steps=args.eval_steps,
        early_stop=args.early_stop,
    )


def load_training_data(data_file: Path, max_negatives: int = 3) -> Tuple[List[InputExample], bool]:
    """
    Load training data from JSONL file.

    Returns:
        - List of InputExample objects
        - Boolean indicating if hard negatives are present
    """
    examples = []
    has_negatives = False

    print(f"Loading training data from {data_file}...")

    with open(data_file, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            if not line.strip():
                continue

            try:
                data = json.loads(line)
            except json.JSONDecodeError as e:
                print(f"Warning: Invalid JSON at line {line_num}: {e}")
                continue

            query = data.get("query", "")
            positives = data.get("pos", [])
            negatives = data.get("neg", [])

            if not query or not positives:
                continue

            # Check if this entry has hard negatives
            if negatives:
                has_negatives = True

            # Create training examples
            for pos in positives:
                if negatives:
                    # With hard negatives: [query, positive, neg1, neg2, ...]
                    # Limit number of negatives
                    neg_subset = negatives[:max_negatives]
                    texts = [query, pos] + neg_subset
                    examples.append(InputExample(texts=texts))
                else:
                    # Without negatives: [query, positive]
                    examples.append(InputExample(texts=[query, pos]))

    print(f"Loaded {len(examples)} training examples")
    print(f"Hard negatives present: {has_negatives}")

    return examples, has_negatives


def load_training_data_multiple_files(data_dir: Path, max_negatives: int = 3) -> Tuple[List[InputExample], bool]:
    """
    Load training data from multiple JSONL files in a directory.
    Falls back to this if combined file doesn't exist.
    """
    examples = []
    has_negatives = False

    for jsonl_file in sorted(data_dir.glob("*.jsonl")):
        # Skip combined file to avoid duplication
        if "combined" in jsonl_file.name:
            continue

        print(f"Loading {jsonl_file.name}...")
        file_examples, file_has_neg = load_training_data(jsonl_file, max_negatives)
        examples.extend(file_examples)
        has_negatives = has_negatives or file_has_neg

    return examples, has_negatives


def load_gold_standard(eval_file: Path) -> Tuple[Dict[str, str], Dict[str, Dict[str, int]], Dict[str, List[str]]]:
    """
    Load gold standard evaluation file for IR evaluation.

    Returns:
        - queries: dict of query_id -> query_text
        - relevant_docs: dict of query_id -> dict of doc_id -> relevance
        - corpus: dict of doc_id -> text (placeholder, actual text loaded separately)
    """
    queries = {}
    relevant_docs = {}

    print(f"Loading gold standard from {eval_file}...")

    with open(eval_file, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            if not line.strip():
                continue

            try:
                data = json.loads(line)
            except json.JSONDecodeError:
                continue

            query_id = f"q{line_num}"
            query_text = data.get("query", "")

            if not query_text:
                continue

            queries[query_id] = query_text

            # Build relevance dict
            rel_dict = {}
            for rel_doc in data.get("relevant", []):
                doc_id = rel_doc.get("id", "")
                relevance = rel_doc.get("relevance", 1)
                if doc_id:
                    rel_dict[doc_id] = relevance

            relevant_docs[query_id] = rel_dict

    print(f"Loaded {len(queries)} evaluation queries")

    return queries, relevant_docs, {}


def create_loss_function(model: SentenceTransformer, has_negatives: bool, use_matryoshka: bool):
    """
    Create appropriate loss function based on data and configuration.

    - MultipleNegativesRankingLoss: Standard contrastive loss, uses in-batch negatives
    - MultipleNegativesRankingLoss with hard negatives: Uses provided hard negatives
    - MatryoshkaLoss: Wraps the base loss for multi-dimension training
    """
    # Base loss - MultipleNegativesRankingLoss works well for both cases
    # When hard negatives are provided, they're treated as additional negatives
    base_loss = losses.MultipleNegativesRankingLoss(model)

    if use_matryoshka:
        # Matryoshka loss trains embeddings that work at multiple dimensions
        # Useful for faster search with minimal quality loss
        from sentence_transformers.losses import MatryoshkaLoss
        matryoshka_dims = [256, 512, 768, 1024]  # BGE-M3 is 1024-dim
        print(f"Using Matryoshka loss with dimensions: {matryoshka_dims}")
        return MatryoshkaLoss(model, base_loss, matryoshka_dims=matryoshka_dims)

    return base_loss


class CheckpointCallback:
    """Callback to evaluate and save checkpoints during training."""

    def __init__(
        self,
        model: SentenceTransformer,
        output_dir: Path,
        eval_queries: Optional[Dict[str, str]] = None,
        eval_relevant: Optional[Dict[str, Dict[str, int]]] = None,
        eval_steps: int = 500,
        early_stop: bool = False,
        patience: int = 3
    ):
        self.model = model
        self.output_dir = output_dir
        self.eval_queries = eval_queries
        self.eval_relevant = eval_relevant
        self.eval_steps = eval_steps
        self.early_stop = early_stop
        self.patience = patience

        self.best_score = 0.0
        self.steps_without_improvement = 0
        self.global_step = 0

    def __call__(self, score, epoch, steps):
        self.global_step += 1

        # Only evaluate at specified intervals
        if self.global_step % self.eval_steps != 0:
            return

        # Save checkpoint
        checkpoint_dir = self.output_dir / "checkpoints" / f"step_{self.global_step}"
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.model.save(str(checkpoint_dir))

        # Log progress
        print(f"\n  [Step {self.global_step}] Checkpoint saved to {checkpoint_dir}")

        # Early stopping check (if evaluation data available)
        if self.eval_queries and score is not None:
            if score > self.best_score:
                self.best_score = score
                self.steps_without_improvement = 0
                print(f"  New best score: {score:.4f}")
            else:
                self.steps_without_improvement += 1
                print(f"  No improvement ({self.steps_without_improvement}/{self.patience})")

                if self.early_stop and self.steps_without_improvement >= self.patience:
                    print(f"  Early stopping triggered!")
                    return True  # Signal to stop training

        return False


def train_model(config: TrainingConfig):
    """Main training function"""
    print("=" * 60)
    print("BGE-M3 Fine-tuning for Arabic Islamic Texts")
    print("(Precision-Optimized Configuration)")
    print("=" * 60)

    # Determine device
    device = config.device if config.device != "auto" else get_device()
    print(f"Device: {device}")

    # Check for training data
    if config.data_file.exists():
        examples, has_negatives = load_training_data(config.data_file, config.hard_negatives)
    elif config.data_file.parent.exists():
        print(f"Combined file not found, loading from directory...")
        examples, has_negatives = load_training_data_multiple_files(
            config.data_file.parent, config.hard_negatives
        )
    else:
        print(f"Error: Data not found at {config.data_file}")
        print("Run the training data generation scripts first:")
        print("  bun run training/scripts/generate-training-pairs-quran.ts")
        print("  bun run training/scripts/generate-training-pairs-hadith.ts")
        print("  bun run training/scripts/combine-training-data.ts")
        sys.exit(1)

    if len(examples) == 0:
        print("Error: No training examples loaded")
        sys.exit(1)

    # Load gold standard for evaluation (if available)
    eval_queries = None
    eval_relevant = None
    if config.eval_file and config.eval_file.exists():
        eval_queries, eval_relevant, _ = load_gold_standard(config.eval_file)

    # Load model
    print(f"\nLoading model: {config.model_name}")
    if config.resume_from and config.resume_from.exists():
        print(f"Resuming from checkpoint: {config.resume_from}")
        model = SentenceTransformer(str(config.resume_from), device=device)
    else:
        model = SentenceTransformer(config.model_name, device=device)

    print(f"Model embedding dimension: {model.get_sentence_embedding_dimension()}")

    # Shuffle examples
    random.shuffle(examples)

    # Create dataloader
    train_dataloader = DataLoader(
        examples,
        shuffle=True,
        batch_size=config.batch_size
    )

    # Create loss function
    train_loss = create_loss_function(model, has_negatives, config.use_matryoshka)

    # Create output directory
    config.output_dir.mkdir(parents=True, exist_ok=True)

    # Calculate warmup steps
    total_steps = len(train_dataloader) * config.epochs
    warmup_steps = int(total_steps * config.warmup_ratio)

    # Print configuration
    print(f"\nTraining configuration (PRECISION-OPTIMIZED):")
    print(f"  Device: {device}")
    print(f"  Batch size: {config.batch_size} (smaller = harder in-batch negatives)")
    print(f"  Epochs: {config.epochs}")
    print(f"  Learning rate: {config.learning_rate} (conservative for fine-tuning)")
    print(f"  Warmup ratio: {config.warmup_ratio} ({warmup_steps} steps)")
    print(f"  Training examples: {len(examples)}")
    print(f"  Steps per epoch: {len(train_dataloader)}")
    print(f"  Total steps: {total_steps}")
    print(f"  Hard negatives: {config.hard_negatives if has_negatives else 'None (using in-batch)'}")
    print(f"  Matryoshka loss: {config.use_matryoshka}")
    print(f"  Evaluation file: {config.eval_file or 'None'}")
    print(f"  Eval steps: {config.eval_steps}")
    print(f"  Early stopping: {config.early_stop}")
    print(f"  Output: {config.output_dir}")
    print()

    # Save training config
    config_path = config.output_dir / "training_config.json"
    with open(config_path, "w") as f:
        json.dump({
            "model_name": config.model_name,
            "batch_size": config.batch_size,
            "epochs": config.epochs,
            "learning_rate": config.learning_rate,
            "warmup_ratio": config.warmup_ratio,
            "warmup_steps": warmup_steps,
            "hard_negatives": config.hard_negatives if has_negatives else 0,
            "use_matryoshka": config.use_matryoshka,
            "num_examples": len(examples),
            "device": device,
            "precision_optimized": True,
        }, f, indent=2)

    # Create callback for checkpoint evaluation
    callback = CheckpointCallback(
        model=model,
        output_dir=config.output_dir,
        eval_queries=eval_queries,
        eval_relevant=eval_relevant,
        eval_steps=config.eval_steps,
        early_stop=config.early_stop,
        patience=config.patience,
    )

    # Train
    print("Starting training...")
    print(f"  Precision target: P@5 > 0.85, MRR > 0.80, FP Rate < 15%")
    print()

    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=config.epochs,
        warmup_steps=warmup_steps,
        output_path=str(config.output_dir),
        show_progress_bar=True,
        checkpoint_save_steps=config.eval_steps,
        checkpoint_path=str(config.output_dir / "checkpoints"),
        optimizer_params={"lr": config.learning_rate},
        callback=callback,
    )

    print(f"\nTraining complete!")
    print(f"Model saved to: {config.output_dir}")

    # Test the model
    print("\nTesting fine-tuned model...")
    test_queries = [
        "إنما الأعمال بالنيات",  # Actions are by intentions (Arabic)
        "الصلاة في وقتها",  # Prayer on time (Arabic)
        "What is the reward for patience?",  # English
        "hadith about charity",  # English keywords
        "آية الكرسي",  # Ayat al-Kursi
        "Surah Al-Fatiha",  # Transliteration
        "pillars of Islam five",  # Keywords
    ]

    for query in test_queries:
        embedding = model.encode(query)
        print(f"  '{query[:40]}...' -> {len(embedding)}-dim vector")

    print("\nDone! To use the fine-tuned model:")
    print(f"  1. Copy {config.output_dir} to your server")
    print("  2. Update embedding-server to use: CUSTOM_WEIGHTS_PATH=<path>")
    print("  3. Regenerate embeddings with: bun run scripts/generate-embeddings.ts --model=bge-m3")

    # Final evaluation reminder
    if config.eval_file:
        print("\nRun precision evaluation:")
        print(f"  bun run training/scripts/evaluate-precision.ts --model=bge-m3")


if __name__ == "__main__":
    config = parse_args()
    train_model(config)
